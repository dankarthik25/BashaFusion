{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff0d55db-3648-40a3-9d7f-8d0f3398f216",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import sys\n",
    "import os\n",
    "# from IAST import IAST\n",
    "# from IastFramework import IAST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0250e3e-f673-4368-9a91-592b1c5e1dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd \n",
    "# # pd.read_sql_table(iast.alphabet, iast.db_connect)\n",
    "# alphabets= pd.read_sql_query(f\"SELECT * FROM {iast.alphabet}\", iast.db_connect)\n",
    "# barakhadi = pd.read_sql_query(f\"SELECT * FROM {iast.barakhadi}\", iast.db_connect)\n",
    "\n",
    "# with pd.ExcelWriter('IASTv2.xlsx', mode=\"w\",engine=\"openpyxl\")as writer:\n",
    "#     alphabets.to_excel(writer, sheet_name=iast.alphabet,index=False)\n",
    "#     barakhadi.to_excel(writer,sheet_name=iast.barakhadi,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf4491ce-d828-4b13-94c4-e50336c31e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import sqlite3\n",
    "# path = 'IASTv3.xlsx'\n",
    "# alphabets = pd.read_excel(path,sheet_name='IndianAlphabet')\n",
    "# barakhadi = pd.read_excel(path,sheet_name='Barakhadi')\n",
    "# inv_map   = pd.read_excel(path,sheet_name='InvAlpha')\n",
    "# inv_bara  = pd.read_excel(path,sheet_name='InvBara')\n",
    "# # Create new database if not exist and replace old talbe\n",
    "# connect = sqlite3.connect('iastv3.db')\n",
    "# alphabets.to_sql('IndianAlphabet', connect, if_exists='replace',index=False)\n",
    "# barakhadi.to_sql('Barakhadi', connect, if_exists='replace',index=False)\n",
    "# inv_map.to_sql('InvAlpha', connect, if_exists='replace',index=False)\n",
    "# inv_bara.to_sql('InvBara', connect, if_exists='replace',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eb0a12f8-5595-4886-8e03-8a22d9639ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import sys\n",
    "\n",
    "\n",
    "class IAST():\n",
    "    vowel_plist=[['r̥̄', 'l̥̄'], \n",
    "                 ['r̥', 'au', 'ai', 'ụ̄ ', 'ạ̄ ', 'oṁ', 'm̐', 'aḥ', 'l̥'], \n",
    "                 ['a', 'ā', 'ạ', 'ụ', 'æ', 'ǣ', 'i', 'ī', 'u', 'ū', 'e', 'ē', 'ê', 'ê',\n",
    "                  'o', 'ǒ', 'ō', 'ô', 'ʻ', 'ḥ', 'ḫ', 'ẖ', 'ṁ', 'ṃ']]\n",
    "    \n",
    "    consonant_list = [['n̆g', 'n̆j', 'n̆ḍ', 'n̆d', 'm̆b', 'k͟h'], \n",
    "                       ['kh', 'g̈', 'gh', 'ch', 'ĉh', 'jh', 'ṭh', 'ḍh', 'dh', 'd̤', \n",
    "                        'ṛh', 'th', 'ph', 'bh', 'b̤', 'ṟ̄', 'y̌', 'r̆', 'l̤', '||'], \n",
    "                       ['ḵ', 'k', 'g', 'ṅ', 'c', 'ĉ', 'j', 'ǰ', 'ĵ', 'ñ', 'ṭ', 'ḍ', 'ḍ', \n",
    "                        'ṛ', 'ṇ', 't', 'd', 'n', 'p', 'b', 'm', 'ṟ', 'ṯ', 'ḏ', 'ṉ',\n",
    "                        'ḻ', 'y', 'ẏ', 'r', 'l', 'ḷ', 'v', 'ś', 'ṣ', 's', 'h', 'q', 'ġ', \n",
    "                        'z', 'z', 'ž', 'ž', 'ž', 'f', 's̱', 's̤', 'h̤', 't̤', 'w',\n",
    "                        'ẕ', 'ż', 'ẓ', 'ẏ', 'ṟ', \n",
    "                        '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '|']]\n",
    "    \n",
    "    # Phonetic Search Static Variables\n",
    "    zero_vowels={ '':['a', \"ā\", \"â\",\"i\", \"ī\",\"u\", \"ū\",chr(805),chr(803),\n",
    "                      \"l̥\", \"l̥̄\",\"e\", \"ē\", \"ê\",\"o\", \"ō\", \"ô\",\n",
    "                      \"ṁ\", \"m̐\", \"ṃ\", \"ṃ\",\"n̆\", \"n̆\", \"n̆\",\"ḥ\" , \"ḫ\", \"ẖ\", \"ḥ\"],\n",
    "                  'r': [\"r̥\", \"r̥̄\"]\n",
    "                } # replacing with r is not working for 'r̥' so we replace with chr(805) above\n",
    "    truncated_vowels = { '':[chr(805), chr(803), chr(772),chr(784),chr(774)],\n",
    "                        'a':[\"ā\", \"â\"], \n",
    "                        'i':[\"i\", \"ī\"], \n",
    "                        'u':[\"u\", \"ū\"], \n",
    "                        'r':[\"r̥\", \"r̥̄\"],\n",
    "                        'l':[\"l̥\", \"l̥̄\"],\n",
    "                        \"e\":[\"e\", \"ē\", \"ê\"],\n",
    "                                # \"ai\", \n",
    "                        \"o\": [\"o\", \"ō\", \"ô\"], \n",
    "                                                # \"au\",\n",
    "                        'm' :[\"ṁ\", \"m̐\", \"ṃ\", \"ṃ\"], \n",
    "                        'n': [\"n̆\", \"n̆\", \"n̆\"], \n",
    "                        'h' :[\"ḥ\" , \"ḫ\", \"ẖ\", \"ḥ\"],\n",
    "                        }\n",
    "    basic_truncated_consonat = {\n",
    "                        'k' : ['ḵ', 'k', 'kh','k͟ha'],\n",
    "                        'g' :['g','g̈','gh','ġ'],\n",
    "                        'n' : ['ṅ','n̆','ñ','ṇ','ṉ'],\n",
    "                        'c' : ['c', 'ĉ','ch','ĉh'],\n",
    "                        'j' : ['j','ǰ', 'ĵ', 'jh'],\n",
    "                        't': ['ṭ','ṭa','t','th','ṯ','t̤'],\n",
    "                        'd' : ['ḍ', 'd̤','ḍ','ḍh','d','dh','ḏ'],\n",
    "                        'p' : ['p', 'ph'],\n",
    "                        'b' : [ 'b', 'b̤', 'bh'],\n",
    "                        'm' : ['m̆' ],\n",
    "                        'r' : ['ṟ', 'r̆'],\n",
    "                        'l' :['ḻ', 'ḷ', 'l̤'],\n",
    "                        'y' : ['y', 'ẏ', 'y̌'],\n",
    "                        's': ['ś', 'ṣ', 's','s̱', 's̤','sh' ],\n",
    "                        \"z\": [\"z\",\"ž\",\"ž\",\"ž\",'ẕ','ẕ','ẓ','ż'],\n",
    "                        'h' : ['h','h̤']\n",
    "                        }\n",
    "    \n",
    "    \n",
    "    def __init__(self,db_path=None,table_name_alpha='IndianAlphabet',table_name_barakadi='Barakhadi',table_name_inv_alpha='InvAlpha',table_name_inv_bara='InvBara' ):\n",
    "        if db_path is None:\n",
    "            # dir_path = os.path.dirname(__file__)\n",
    "            dir_path = os.getcwd()\n",
    "            self.db_path=os.path.join(dir_path,'iast-token.db')\n",
    "#            self.db_connect = sqlite3.connect(os.path.join(dir_path,'iast-token.db'))\n",
    "        else:\n",
    "            self.db_path=db_path\n",
    "\n",
    "#        print('db_path: ',self.db_path,type(self.db_path))\n",
    "\n",
    "        self.db_connect = sqlite3.connect(self.db_path)\n",
    "        self.alphabet = table_name_alpha\n",
    "        self.barakhadi = table_name_barakadi\n",
    "        self.inv_alphabet = table_name_inv_alpha\n",
    "        self.inv_barakhadi = table_name_inv_bara\n",
    "        self.halant_list = self.get_halant_list() #  ['्', '্', '્', '್', '്', '୍', '్']\n",
    "# phonetic search algo inicialization\n",
    "    def set_query(self,query):\n",
    "        db_cursor = self.db_connect.cursor()\n",
    "        try :\n",
    "            db_cursor.execute(query)    \n",
    "        except sqlite3.Error as e:\n",
    "            print('SQLite error: %s' % (' '.join(e.args)))\n",
    "            print(\"Exception class is: \", e.__class__)\n",
    "            print('SQLite traceback: ')\n",
    "            exc_type, exc_value, exc_tb = sys.exc_info()\n",
    "            # data = []\n",
    "        \n",
    "        finally:\n",
    "            self.db_connect.commit()\n",
    "            # auto_increment and delete on cascade https://stackoverflow.com/questions/29037793/sqlite-integrityerror-unique-constraint-failed\n",
    "            sefl.db_cursor.close()\n",
    "            # db_cursor.close()\n",
    "            # print('Read Query!')\n",
    "        # return data\n",
    "    \n",
    "    def get_query(self,query): # all query which will give or get the data or query which will return some value\n",
    "        db_cursor = self.db_connect.cursor()\n",
    "        try :\n",
    "            db_cursor.execute(query)    \n",
    "            columns = [column[0] for column in db_cursor.description]\n",
    "            data = [dict(zip(columns, row)) for row in db_cursor.fetchall()]\n",
    "            # self.db_connect.close()\n",
    "        except sqlite3.Error as e:\n",
    "            print('SQLite error: %s' % (' '.join(e.args)))\n",
    "            print(\"Exception class is: \", e.__class__)\n",
    "            print('SQLite traceback: ')\n",
    "            exc_type, exc_value, exc_tb = sys.exc_info()\n",
    "            data = []\n",
    "        \n",
    "        finally:\n",
    "            # db_connect.commit()\n",
    "            # auto_increment and delete on cascade https://stackoverflow.com/questions/29037793/sqlite-integrityerror-unique-constraint-failed\n",
    "            db_cursor.close()\n",
    "            # db_cursor.close()\n",
    "            # print('Read Query!')\n",
    "        return data\n",
    "    \n",
    "    def get_iast_idx_query(letter,talbe_name): # query: given indic letter will return iast mapped value \n",
    "        query =f\"\"\"SELECT * FROM {talbe_name} WHERE Devanagari == '{letter}' OR Bengali–Assamese == '{letter}' OR Gujarati == '{letter}' OR Kannada == '{letter}' OR Malayalam == '{letter}' OR Odia == '{letter}' OR Tamil == '{letter}' OR Telugu == '{letter}';\"\"\"\n",
    "\n",
    "        return query \n",
    "\n",
    "    def get_halant_list (self):\n",
    "        letter = 'ŭ'\n",
    "        query = f\"SELECT * FROM {self.barakhadi} WHERE IAST='{letter}'\" \n",
    "        data =self.get_query(query)\n",
    "        # print(data)\n",
    "        del data[0]['IAST']\n",
    "        halant_list = []\n",
    "        for value in data[0].values():\n",
    "            if value is not None and value not in halant_list:\n",
    "                halant_list.append(value)\n",
    "        # print(halant_list)\n",
    "        return halant_list\n",
    "\n",
    "    # word = 'ధృత్రాష్ట్ర ఉవాచ'\n",
    "    def to_iast(self,word): # arg can be word, sentance, line, para, whole doc\n",
    "        output_token = ''\n",
    "        for letter in word: # word\n",
    "            query = IAST.get_iast_idx_query(letter,self.alphabet)    \n",
    "            alpha_token =self.get_query(query)\n",
    "            \n",
    "            query = IAST.get_iast_idx_query(letter,self.barakhadi)    \n",
    "            barakhadi_token =self.get_query(query)\n",
    "            \n",
    "            if len(alpha_token) !=0:\n",
    "                output_token += alpha_token[0]['IAST']\n",
    "                # print(alpha_token[0]['IAST'],end='')\n",
    "            elif letter in  self.halant_list and output_token[-1] in \"a\": \n",
    "                        # >> 'क्' = 'क ' +'्'   # >>> ka + halant = k\n",
    "                output_token = output_token[:-1]\n",
    "            elif len(barakhadi_token) !=0 and output_token[-1] in \"a\":\n",
    "                        #  'कि'='क ' + 'ि' = ka + i => ki\n",
    "                output_token = output_token[:-1]+ barakhadi_token[0]['IAST']\n",
    "            elif len(barakhadi_token) !=0 :\n",
    "                        #   ' किं ' = ='क ' + 'ि' + 'ं'  = ka + i + aṁ = kiṁ\n",
    "                output_token += barakhadi_token[0]['IAST'].replace(\"a\",'')\n",
    "            elif len(barakhadi_token) ==0  and len(alpha_token) ==0 and ord(letter)==8205: # cleaing data\n",
    "                pass\n",
    "            else:\n",
    "                # print(f\"\"\"NOT Present in alpha and barakadi{letter}=={ord(letter)} \"\"\")\n",
    "                output_token += letter\n",
    "            # print(out)\n",
    "            # print(f'{letter}\\t| {output_token}')\n",
    "        # print(f'{word}\\t| {output_token}')\n",
    "        return output_token\n",
    "    # iast.to_iast( word)\n",
    "\n",
    "    def debug_letterbyletter(self,text):\n",
    "    # text =' ധർമക്ഷേത്രേ കുരുക്ഷേത്രേ സമവേതാ യുയുത്സവഃ ।' #\t| dhaർmakṣētrē kurukṣētrē samavētā yuyutsavaḥ |  \n",
    "        for letter in text: \n",
    "            query = IAST.get_iast_idx_query(letter,self.alphabet)    \n",
    "            alpha_token =self.get_query(query)\n",
    "            \n",
    "            query = IAST.get_iast_idx_query(letter,self.barakhadi)    \n",
    "            barakhadi_token =self.get_query(query)\n",
    "            \n",
    "            output_token=''\n",
    "            if len(alpha_token) !=0:\n",
    "                output_token += alpha_token[0]['IAST']\n",
    "                # print(alpha_token[0]['IAST'],end='')\n",
    "            elif len(barakhadi_token) !=0 :\n",
    "                output_token = output_token[:-1]+ barakhadi_token[0]['IAST']\n",
    "            else:\n",
    "                output_token = letter\n",
    "                \n",
    "            output = f'letter= {letter} \\t| ascii(letter) ={ord(letter[0])}\\t| iast = {output_token}' #  | ascii(iast)={ord(iast_letter)}'\n",
    "            print(output)\n",
    "\n",
    "# Methods for phonetic search\n",
    "    # Replace many to one\n",
    "    \n",
    "    def replace_m2o(text, source=None, dest=None): # \n",
    "        if isinstance(source, list):\n",
    "            for source_letter in source:\n",
    "                text = text.replace(source_letter,dest)\n",
    "        elif isinstance(source, str):\n",
    "            text = text.replace(source,dest)\n",
    "        # print(text)\n",
    "        return text\n",
    "        \n",
    "    # Replace many to many\n",
    "    def replace_m2m(output_data,info_dict):\n",
    "        for dest in info_dict.keys():\n",
    "            source = info_dict[dest]\n",
    "            output_data = IAST.replace_m2o(output_data, source=source, dest=dest)\n",
    "        return output_data\n",
    "\n",
    "    # Basic Stemming\n",
    "    def basic_hash(iast_text): # if text is in hin,kan,tel,mal,guj,..etc need to convert to iast \n",
    "        basic_stem_dict = IAST.zero_vowels\n",
    "        basic_stem_dict.update(IAST.basic_truncated_consonat)\n",
    "        output =IAST.replace_m2m(iast_text,basic_stem_dict)\n",
    "        return output\n",
    "        \n",
    "    # Normal Stemming\n",
    "    def normal_hash(iast_text):\n",
    "        normal_stem_dict = IAST.truncated_vowels\n",
    "        normal_stem_dict.update(IAST.basic_truncated_consonat)\n",
    "        output = IAST.replace_m2m(iast_text,normal_stem_dict)\n",
    "        return output\n",
    "    \n",
    "    def get_indic_symbol_query(iast_letter,language,table_name): # query given indic letter will return iast mapped value \n",
    "        # query =f\"\"\"\n",
    "        # SELECT type {language} FROM {table_name} \n",
    "        # WHERE IAST LIKE '{iast_letter}%';\n",
    "        # \"\"\"\n",
    "        query =f\"\"\"SELECT type, IAST, {language}  FROM {table_name} \n",
    "        WHERE IAST LIKE '{iast_letter}%';\n",
    "        \"\"\"\n",
    " # 'type': 'consonants',\n",
    " #  'IAST': 'gha',\n",
    " #  'Devanagari': 'घ',\n",
    " #  'Bengali–Assamese': 'ঘ',\n",
    " #  'Gujarati': 'ઘ',\n",
    " #  'Gurmukhi': 'ਘ',\n",
    "        # print(query.replace('\\n','').replace(\"  \",\"\")) \n",
    "    # SELECT * FROM IndianAlphabet WHERE Devanagari == 'ध' OR Bengali–Assamese == 'ध' OR Gujarati == 'ध' OR Kannada == 'ध' OR Malayalam == 'ध' OR Odia == 'ध' OR Tamil == 'ध' OR Telugu == 'ध'    \n",
    "        return query \n",
    "        \n",
    "# iast to indic language(any lang) \n",
    "    \n",
    "    def lex_iast(keyword, word):\n",
    "        tokens=[]\n",
    "        # print(keyword,word)\n",
    "        slic_pstart = 0 # previous start point\n",
    "        # slic_pstart = 0 # previous start point\n",
    "        # slic/_pstart = 0 # previous start point\n",
    "        \n",
    "        len_word = len(word)\n",
    "        slic3_flag = False\n",
    "        slic2_flag = False\n",
    "        slic1_flag = False\n",
    "        \n",
    "        for idx, letter in enumerate(word):\n",
    "            slic3 = word[idx:idx+3]\n",
    "            if slic3 in keyword[0]:\n",
    "                slic3_flag=True\n",
    "            else:\n",
    "                slic3_flag=False\n",
    "            slic2 = word[idx:idx+2]            \n",
    "            if slic2 in keyword[1]:\n",
    "                slic2_flag=True\n",
    "            else:\n",
    "                slic2_flag=False        \n",
    "            slic1 = word[idx:idx+1]        \n",
    "            if slic1 in keyword[2]:\n",
    "                slic1_flag=True\n",
    "            else:\n",
    "                slic1_flag=False\n",
    "        \n",
    "            if slic3_flag:\n",
    "                if slic_pstart < idx:\n",
    "                    # print(f'Append missing data btw idx slic3 {slic_pstart}:{idx} {word[slic_pstart:idx]}' )                                                    \n",
    "                    tokens.append(word[slic_pstart:idx])\n",
    "                tokens.append(slic3)                \n",
    "                slic_pstart=idx+1 + len(slic3)-1\n",
    "                # print(f'At index {idx} :Need to split3 at {slic3}' )\n",
    "            else:\n",
    "                if slic2_flag:\n",
    "                    if slic_pstart < idx:\n",
    "                        # print(f'Append missing data btw idx slic2 {slic_pstart}:{idx} {word[slic_pstart:idx]}' )                                    \n",
    "                        tokens.append(word[slic_pstart:idx])            \n",
    "                    # if slic_pstart <idx+1:\n",
    "                    tokens.append(slic2)\n",
    "                    slic_pstart=idx+len(slic2)       \n",
    "                    # print(f'slic2 pstart:{slic_pstart}')\n",
    "                    # print(f'At index {idx} :Need to split2 at {slic2} and set next start point: {idx+1+len(slic2)} and it value:{word[idx+len(slic2)]}' )                \n",
    "                else:\n",
    "                    if slic1_flag:\n",
    "                        if slic_pstart < idx:\n",
    "                            tokens.append(word[slic_pstart:idx])\n",
    "                            # print(f'Append missing data btw idx slic1 {slic_pstart}:{idx} {word[slic_pstart:idx]}' )                                    \n",
    "                        if slic_pstart<=idx:\n",
    "                            tokens.append(slic1)\n",
    "                            slic_pstart=idx+1 + len(slic1)-1\n",
    "                            # print(f'At index {idx} :Need to split1 at {slic2}' )                                    \n",
    "        return tokens\n",
    "\n",
    "    def iast2tokens(word):\n",
    "        if len(word) <=1:\n",
    "            return word\n",
    "    # def iast2tokens(vowel_plist,consonant_list,  word):        \n",
    "        vowel_plist=IAST.vowel_plist\n",
    "        consonant_list=IAST.consonant_list\n",
    "\n",
    "        iast_tokens= []\n",
    "        vowel_tokens = IAST.lex_iast(vowel_plist,word)\n",
    "        # print(vowel_tokens)\n",
    "        if word[-1*len(vowel_tokens[-1]):]==vowel_tokens[-1]:\n",
    "            pass\n",
    "            # print('Last word match with vowel no need to append')\n",
    "        else:\n",
    "            vowel_tokens.append(word.split(vowel_tokens[-1])[-1])\n",
    "            # print('need to append')\n",
    "            # print(vowel_tokens)\n",
    "        for i in vowel_tokens:\n",
    "            # print(i, lex_iast(consonant_list,i))\n",
    "            if len(IAST.lex_iast(consonant_list,i)) <=1:\n",
    "                iast_tokens.append(i)\n",
    "            else:\n",
    "                iast_tokens.extend(IAST.lex_iast(consonant_list,i))\n",
    "        return iast_tokens\n",
    "\n",
    "\n",
    "    def tokens2dict_tokenes(self,tokens,indic_lang):\n",
    "        input_tokens=''\n",
    "        output_string = []\n",
    "        for token in tokens:\n",
    "            # query_bara = f\"\"\"SELECT IAST,{indic_lang} FROM {self.barakhadi} WHERE IAST LIKE '%{token}'\"\"\"\n",
    "            # query_alpha = f\"\"\"SELECT type, IAST,{indic_lang} FROM {self.alphabet} WHERE IAST LIKE '{token}%'\"\"\"\n",
    "            query_bara = f\"\"\"SELECT IAST,{indic_lang} FROM {self.inv_barakhadi} WHERE IAST ='{token}'\"\"\"\n",
    "            query_alpha = f\"\"\"SELECT type, IAST,{indic_lang} FROM {self.inv_alphabet} WHERE IAST='{token}'\"\"\"\n",
    "\n",
    "            # print(query_bara)\n",
    "            # print(query_alpha)\n",
    "            data_alpha = self.get_query(query_alpha)\n",
    "            data_bara = self.get_query(query_bara)\n",
    "            # print(data_bara)\n",
    "            input_tokens += token+ ' '\n",
    "            # print(token,'Alphabets: ', data_alpha,' Barakadi',data_bara)\n",
    "            temp_dic = dict()\n",
    "            temp_dic['IAST']=token\n",
    "            temp_dic['lang']=indic_lang\n",
    "        # type, alph, bara are enter below \n",
    "            # temp_dic['type']=\n",
    "            # temp_dic['alph']=\n",
    "            # temp_dic['bara']= \n",
    "            if len(data_alpha):\n",
    "                temp_dic['type']=data_alpha[0]['type']\n",
    "                # temp_dic['alph']=data_alpha[0][indic_lang] # wrong method if token = n ,n̆ḍa, n̆ja then : \n",
    "                # we 1st search result is none which we need to filter\n",
    "                for entry in data_alpha: \n",
    "                    if entry[indic_lang] is not None :                        \n",
    "                        # print(entry[indic_lang],entry['IAST'],entry['type'])\n",
    "                        temp_dic['alph']=entry[indic_lang]\n",
    "                        temp_dic['type']=entry['type']\n",
    "                        break  \n",
    "                    else:\n",
    "                        # print(f'In entry:{entry} indic_lang: {indic_lang} is None need to update dic')\n",
    "                        temp_dic['alph']=entry[indic_lang]\n",
    "                        temp_dic['type']=entry['type']\n",
    "                        break\n",
    "    \n",
    "                # output_string +=' | '+ data_alpha[0][indic_lang]+' : '  +data_alpha[0]['type'] +' | '\n",
    "            else:\n",
    "                temp_dic['alph']=None\n",
    "            if len(data_bara):\n",
    "                # output_string +=' | '+ data_bara[0][indic_lang] +' | '\n",
    "                temp_dic['type']='vowel'\n",
    "                temp_dic['bara']=data_bara[0][indic_lang]\n",
    "            else:\n",
    "                temp_dic['bara']=None\n",
    "            output_string.append(temp_dic)\n",
    "        return output_string\n",
    "        \n",
    "    def get_indic_halant(self,indic_lang):\n",
    "        query_alpha = f\"\"\"SELECT IAST,{indic_lang} FROM {self.barakhadi} WHERE IAST='ŭ'\"\"\"\n",
    "        data_alpha = self.get_query(query_alpha)\n",
    "        halant = self.get_query(query_alpha)[0][indic_lang]\n",
    "        return halant\n",
    "\n",
    "    \n",
    "    def dict_tokens2indic(output_string,halant):\n",
    "                            #'क'+  'ा'+'ः' #>>>  'काः'\n",
    "                            #'क'+'्'   # >>> 'क्'\n",
    "                            #'क्'+ 'ा' # >>> 'क्ा'\n",
    "                            \n",
    "        output=''\n",
    "        for idx, item in enumerate(output_string):\n",
    "            print_status = False    \n",
    "            # print(idx, item)\n",
    "            if idx ==0:\n",
    "                prev_item=dict()\n",
    "            else:\n",
    "                prev_item=output_string[idx-1]\n",
    "            if idx < len(output_string)-1:\n",
    "                \n",
    "                next_item = output_string[idx+1]\n",
    "            elif idx ==len(output_string)-1:\n",
    "                next_item = dict()\n",
    "                \n",
    "            if 'type' in item.keys() and item['type']=='consonants':\n",
    "                if 'type' in next_item.keys() and next_item['type']=='vowel':\n",
    "                    # print(item['alph'], end=\" \")\n",
    "                    if item['alph'] is not None :\n",
    "                        output +=item['alph']\n",
    "                        print_status =True\n",
    "                elif 'type' in next_item.keys() and next_item['type']=='consonants':\n",
    "                    # print(item['alph']+halant,end=\"\")\n",
    "                    if item['alph'] is not None :\n",
    "                        output +=item['alph']+halant\n",
    "                        print_status =True\n",
    "                elif 'type' not in next_item: # word ending with consonant and halant\n",
    "                    if item['alph'] is not None :\n",
    "                        output +=item['alph']+halant\n",
    "                        print_status =True\n",
    "                    \n",
    "                    \n",
    "            if 'type' in item.keys() and item['type']=='vowel':\n",
    "                # print('ITEM: ',item)\n",
    "                # print('PREV ITEM: ',prev_item)\n",
    "                if 'type' in prev_item.keys() and prev_item['type']=='consonants':\n",
    "                    # print(item['bara'], end=' ')\n",
    "                    if item['IAST']=='a':\n",
    "                        print_status =True                            \n",
    "                        pass\n",
    "                    else:        \n",
    "                        if item['bara'] is not None :                                            \n",
    "                            output +=item['bara']\n",
    "                            print_status =True            \n",
    "                    # print(item)\n",
    "\n",
    "                if 'type' in prev_item.keys() and prev_item['type']=='vowel':\n",
    "                    if item['bara'] is not None :                    \n",
    "                        output +=item['bara']            \n",
    "                        # print(item)\n",
    "                        print_status =True\n",
    "                # pass\n",
    "                if 'type' not in prev_item : # starting of word or starting of line\n",
    "                    if item['alph'] is not None :\n",
    "                        output +=item['alph']                            \n",
    "                        print_status =True                \n",
    "        \n",
    "            \n",
    "            if not print_status:\n",
    "                output +=item['IAST']\n",
    "            # print(output)\n",
    "        return output\n",
    "    \n",
    "    # def iast2indic(self,vowel_plist,consonant_list,word,indic_lang):    \n",
    "    def iast2indic(self,word,indic_lang):\n",
    "        vowel_plist=IAST.vowel_plist\n",
    "        consonant_list=IAST.consonant_list\n",
    "        if len(word)==0:\n",
    "            return word\n",
    "        tokens= IAST.iast2tokens(word)\n",
    "        # tokens= IAST.iast2tokens(vowel_plist,consonant_list,  word)\n",
    "        # print(tokens)\n",
    "        dict_tokene_list = self.tokens2dict_tokenes(tokens,indic_lang)\n",
    "        # print(output_string)\n",
    "        # halant=self.get_indic_halant(indic_lang)\n",
    "        query_alpha = f\"\"\"SELECT IAST,{indic_lang} FROM {self.inv_barakhadi} WHERE IAST='ŭ';\"\"\"\n",
    "        data_alpha = self.get_query(query_alpha)\n",
    "        halant = self.get_query(query_alpha)[0][indic_lang]\n",
    "        # print(halant)\n",
    "        output=IAST.dict_tokens2indic(dict_tokene_list,halant)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c056934a-c812-44c0-93da-fdfce19f9aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "iast = IAST(db_path='iastv3.db')\n",
    "# f\"\"\"SELECT IAST,{indic_lang} FROM {iast.inv_barakhadi} WHERE IAST='ŭ';\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "837dd6b2-64dc-46d9-960f-0d3c407e44d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# iast.db_path\n",
    "print(iast.get_indic_halant('Tamil'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "015623fc-66dc-44d8-9adc-bf251d348f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "కం n̆g ధృతరాష్ట్ర ఉవాచ ఇతాః కిం  యుయుత్సవః  పాణ్డవానీకం ఇతాః కిం ఆం  ఈం  కిం యుయుత్సుం రాన్సఖీంస్తథా\n"
     ]
    }
   ],
   "source": [
    "word = 'kaṁ n̆g dhr̥tarāṣṭra uvāca itāḥ kiṁ  yuyutsavaḥ  pāṇḍavānīkaṁ itāḥ kiṁ āṁ  īṁ  kiṁ yuyutsuṁ rānsakhīṁstathā'\n",
    "indic_lang = 'Telugu' # 'Kannada' # 'Telugu'\n",
    "# def iast2indic(iast,vowel_plist,consonant_list,word,indic_lang):\n",
    "tokens= IAST.iast2tokens( word)\n",
    "\n",
    "dict_tokene_list = iast.tokens2dict_tokenes(tokens,indic_lang)\n",
    "# print(output_string)\n",
    "halant=iast.get_indic_halant(indic_lang)\n",
    "\n",
    "output=IAST.dict_tokens2indic(dict_tokene_list,halant)\n",
    "# print(word)\n",
    "# print(tokens)\n",
    "# print(halant)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b7898473-738f-4fbd-b009-a8c9c80c7c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ஆலாயால தற வேணஂ அடுததொரமபலஂ வேணஂ\n",
      "ஆலிநு சேரநநொரு குளவுஂ வேணஂ \n",
      "குளிபபாநாய குளஂ வேணஂ  குளததில செநதாமர வேணஂ \n",
      "குளிசச செநநகஂ புககாந சநdநஂ வேணஂ \n",
      "பூவாயால மணஂ வேணஂ  பூமாநாயால gுணஂ வேணஂ \n",
      "பூமாநிநிமாரகḷ அடககஂ வேணஂ \n",
      "யுddhததிஙகல ராமந நலலூ, குலததிஙகல ஸீத நலலூ \n",
      "ஊணுறககமுபேகஷிககாந லகஷமணந நலலூ \n",
      "படயகக bhரதந நலலூ, பறவாந பைஙகிளி நலலூ \n",
      "பறககுநந பகஷிகளில gருḍhந நலலூ \n",
      "நாடாயால நr̥பந வேணஂ  அரிகில மநதரிமார வேணஂ \n",
      "நாடிநு gூணமுளள பரஜகள வேணஂ.. \n",
      "மஙஙாடடசசநு நயாயஂ நலலூ மஂgலயததிநு ஸவரணணே நலலூ \n",
      "மஙஙாதிரிபபாந நிலவிளகக நலலூ. \n",
      "பாலயததசசநுபாயஂ நலலூ பாலில பஞசஸார நலலூ \n",
      "பாராதிரிபபாந சில பdவி நலலூ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"ālāyāl taṟa vēṇaṁ aṭuttorampalaṁ vēṇaṁ\n",
    "ālinu cērnnoru kuḷavuṁ vēṇaṁ \n",
    "kuḷippānāy kuḷaṁ vēṇaṁ  kuḷattil centāmara vēṇaṁ \n",
    "kuḷicc cennakaṁ pukkān candanaṁ vēṇaṁ \n",
    "pūvāyāl maṇaṁ vēṇaṁ  pūmānāyāl guṇaṁ vēṇaṁ \n",
    "pūmāninimārkaḷ aṭakkaṁ vēṇaṁ \n",
    "yuddhattiṅkal rāman nallū, kulattiṅkal sīta nallū \n",
    "ūṇuṟakkamupēkṣikkān lakṣmaṇan nallū \n",
    "paṭaykk bharatan nallū, paṟavān paiṅkiḷi nallū \n",
    "paṟakkunna pakṣikaḷil garuḍhan nallū \n",
    "nāṭāyāl nr̥pan vēṇaṁ  arikil mantrimār vēṇaṁ \n",
    "nāṭinu gūṇamuḷḷa prajakaḷ vēṇaṁ.. \n",
    "maṅṅāṭṭaccanu nyāyaṁ nallū maṁgalyattinu svarṇṇē nallū \n",
    "maṅṅātirippān nilaviḷakk nallū. \n",
    "pālyattaccanupāyaṁ nallū pālil pañcasāra nallū \n",
    "pārātirippān cila padavi nallū \n",
    "\"\"\"\n",
    "# working code \n",
    "# indic_lang='Devanagari'\n",
    "# indic_lang='Kannada'\n",
    "# indic_lang='Telugu'\n",
    "\n",
    "# code is not working for below languages\n",
    "# indic_lang='Odia'\n",
    "indic_lang='Tamil'\n",
    "# indic_lang='Bengali–Assamese'\n",
    "# print(iast.iast2indic(text,indic_lang))\n",
    "\n",
    "print_list =[]\n",
    "for line in text.split('\\n'):\n",
    "    line = line.strip()\n",
    "    # print(f'\\n\\t\\t\\t line : {line}')\n",
    "    # print('\\n',line)\n",
    "    # print(line.split(\" \"))\n",
    "    output_list =[]    \n",
    "    for word in line.split(\" \"):\n",
    "        output_word = ''\n",
    "        # print(f'word: {word}')\n",
    "        # print('\\nword:',word, 'len of word:',len(word))\n",
    "        output_word =iast.iast2indic(word,indic_lang)\n",
    "        output_list.append(output_word)\n",
    "        # print(f'\\t\\toutput_list:{output_list}')\n",
    "        # print(f'\\t\\tPrint_Output:{print_output}')        \n",
    "    print_list.append(' '.join(output_list))\n",
    "    # print_output += output + \"\\n\"\n",
    "    # print(print_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0dc788db-b5b6-4186-a401-c6902ddec994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ஆலாயால தற வேணஂ அடுததொரமபலஂ வேணஂ\n",
      "ஆலிநு சேரநநொரு குளவுஂ வேணஂ\n",
      "குளிபபாநாய குளஂ வேணஂ  குளததில செநதாமர வேணஂ\n",
      "குளிசச செநநகஂ புககாந சநdநஂ வேணஂ\n",
      "பூவாயால மணஂ வேணஂ  பூமாநாயால gுணஂ வேணஂ\n",
      "பூமாநிநிமாரகள அடககஂ வேணஂ\n",
      "யுddhததிஙகல ராமந நலலூ, குலததிஙகல ஸீத நலலூ\n",
      "ஊணுறககமுபேகஷிககாந லகஷமணந நலலூ\n",
      "படயகக bhரதந நலலூ, பறவாந பைஙகிளி நலலூ\n",
      "பறககுநந பகஷிகளில gருḍhந நலலூ\n",
      "நாடாயால நr̥பந வேணஂ  அரிகில மநதரிமார வேணஂ\n",
      "நாடிநு gூணமுளள பரஜகள வேணஂ..\n",
      "மஙஙாடடசசநு நயாயஂ நலலூ மஂgலயததிநு ஸவரணணே நலலூ\n",
      "மஙஙாதிரிபபாந நிலவிளகக நலலூ.\n",
      "பாலயததசசநுபாயஂ நலலூ பாலில பஞசஸார நலலூ\n",
      "பாராதிரிபபாந சில பdவி நலலூ\n",
      "\n",
      "ஆலாயால தற வேணஂ அடுததொரமபலஂ வேணஂ\n",
      "ālāyāla taṟa vēṇṁ aṭutatoramapalṁ vēṇṁ\n",
      "\n",
      "ஆலிநு சேரநநொரு குளவுஂ வேணஂ\n",
      "ālinu cērananoru kuḷavuṁ vēṇṁ\n",
      "\n",
      "குளிபபாநாய குளஂ வேணஂ  குளததில செநதாமர வேணஂ\n",
      "kuḷipapānāya kuḷṁ vēṇṁ  kuḷatatila cenatāmara vēṇṁ\n",
      "\n",
      "குளிசச செநநகஂ புககாந சநdநஂ வேணஂ\n",
      "kuḷicaca cenanakṁ pukakāna canadnṁ vēṇṁ\n",
      "\n",
      "பூவாயால மணஂ வேணஂ  பூமாநாயால gுணஂ வேணஂ\n",
      "pūvāyāla maṇṁ vēṇṁ  pūmānāyāla guṇṁ vēṇṁ\n",
      "\n",
      "பூமாநிநிமாரகள அடககஂ வேணஂ\n",
      "pūmāninimārakaḷa aṭakakṁ vēṇṁ\n",
      "\n",
      "யுddhததிஙகல ராமந நலலூ, குலததிஙகல ஸீத நலலூ\n",
      "yuddhtatiṅakala rāmana nalalū, kulatatiṅakala sīta nalalū\n",
      "\n",
      "ஊணுறககமுபேகஷிககாந லகஷமணந நலலூ\n",
      "ūṇuṟakakamupēkaṣikakāna lakaṣamaṇana nalalū\n",
      "\n",
      "படயகக bhரதந நலலூ, பறவாந பைஙகிளி நலலூ\n",
      "paṭayakaka bhratana nalalū, paṟavāna paiṅakiḷi nalalū\n",
      "\n",
      "பறககுநந பகஷிகளில gருḍhந நலலூ\n",
      "paṟakakunana pakaṣikaḷila gruḍhna nalalū\n",
      "\n",
      "நாடாயால நr̥பந வேணஂ  அரிகில மநதரிமார வேணஂ\n",
      "nāṭāyāla nar̥pana vēṇṁ  arikila manatarimāra vēṇṁ\n",
      "\n",
      "நாடிநு gூணமுளள பரஜகள வேணஂ..\n",
      "nāṭinu gūṇamuḷaḷa parajakaḷa vēṇṁ..\n",
      "\n",
      "மஙஙாடடசசநு நயாயஂ நலலூ மஂgலயததிநு ஸவரணணே நலலூ\n",
      "maṅaṅāṭaṭacacanu nayāyṁ nalalū mṁglayatatinu savaraṇaṇē nalalū\n",
      "\n",
      "மஙஙாதிரிபபாந நிலவிளகக நலலூ.\n",
      "maṅaṅātiripapāna nilaviḷakaka nalalū.\n",
      "\n",
      "பாலயததசசநுபாயஂ நலலூ பாலில பஞசஸார நலலூ\n",
      "pālayatatacacanupāyṁ nalalū pālila pañacasāra nalalū\n",
      "\n",
      "பாராதிரிபபாந சில பdவி நலலூ\n",
      "pārātiripapāna cila padvi nalalū\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join(print_list))\n",
    "for line in print_list:\n",
    "    print(line)\n",
    "    print(iast.to_iast(line))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09dc7a97-4961-4cb9-85c9-35c634d9106b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "letter= க \t| ascii(letter) =2965\t| iast = ka\n",
      "letter= ் \t| ascii(letter) =3021\t| iast = ்\n",
      "letter=   \t| ascii(letter) =32\t| iast =  \n",
      "letter= க \t| ascii(letter) =2965\t| iast = ka\n",
      "letter= ் \t| ascii(letter) =3021\t| iast = ்\n",
      "letter= ஷ \t| ascii(letter) =2999\t| iast = ṣa\n",
      "letter= ் \t| ascii(letter) =3021\t| iast = ்\n"
     ]
    }
   ],
   "source": [
    "iast.debug_letterbyletter('க் க்ஷ்')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac71f060-30ec-4857-aeda-dfc692753079",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
